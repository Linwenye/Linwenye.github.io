---
layout: post
title: Tree-Supervised Auxiliary Online Knowledge Distillation. 
conference:  In IJCNN.
authors: Wenye Lin, Yangning Li, Yifeng Ding, Hai-tao Zheng (2022).
git: https://github.com/Linwenye/Tree-Supervised
site: https://arxiv.org/abs/2208.10068
---

We design tree-structured auxiliary (TSA) online knowledge distillation to perform one-stage distillation when the teacher is unavailable. With TSA, we gain an average of 3% to 4% improvement in accuracy on CIFAR-100. On ImageNet, ResNet-34 obtains 74.97% accuracy, which is 1.8% above the vanilla one. On IWSLT translation tasks, we gain an average of 0.9 BLEU improvement over vanilla Transformer for three datasets.