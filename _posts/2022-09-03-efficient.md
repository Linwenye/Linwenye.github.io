---
layout: post
title: Efficient Structured Knowledge Distillation. 
conference:  In arxiv.
authors: Wenye Lin, Yangming Li, Lemao Liu, Shuming Shi, Hai-tao Zheng (2022).
git: https://github.com/Linwenye/Efficient-KD
site: https://arxiv.org/abs/2203.04825
---

Performing knowledge distillation for structured prediction models models is not trivial due to their exponentially large output space. In this work, we propose an approach that is much simpler in its formulation and far more efficient for training than existing approaches. Specifically, we transfer the knowledge from a teacher model to its student model by locally matching their predictions on all sub-structures, instead of the whole output space.
